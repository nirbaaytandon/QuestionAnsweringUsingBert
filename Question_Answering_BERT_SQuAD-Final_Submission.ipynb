{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question answering comes in many forms. In this example, we’ll look at the particular type of extractive QA that involves answering a question about a passage by highlighting the segment of the passage that answers the question. This involves fine-tuning a model which predicts a start position and an end position in the passage. We will use the Stanford Question Answering Dataset (SQuAD) 2.0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites: \n",
    "\n",
    "1. Download and install the required libraries below.\n",
    "2. Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kppY3LDeDksP",
    "outputId": "a027fbe0-50d1-427f-f8d4-face8995ef33",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==1.7.1 in /home/nirbhay/anaconda3/lib/python3.8/site-packages (1.7.1)\n",
      "Requirement already satisfied: numpy in /home/nirbhay/anaconda3/lib/python3.8/site-packages (from torch==1.7.1) (1.18.5)\n",
      "Requirement already satisfied: typing-extensions in /home/nirbhay/anaconda3/lib/python3.8/site-packages (from torch==1.7.1) (3.7.4.3)\n",
      "Requirement already satisfied: pytorch-lightning==1.1.2 in /home/nirbhay/anaconda3/lib/python3.8/site-packages (1.1.2)\n",
      "Requirement already satisfied: PyYAML>=5.1 in /home/nirbhay/anaconda3/lib/python3.8/site-packages (from pytorch-lightning==1.1.2) (5.3.1)\n",
      "Requirement already satisfied: future>=0.17.1 in /home/nirbhay/anaconda3/lib/python3.8/site-packages (from pytorch-lightning==1.1.2) (0.18.2)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /home/nirbhay/anaconda3/lib/python3.8/site-packages (from pytorch-lightning==1.1.2) (1.18.5)\n",
      "Requirement already satisfied: fsspec>=0.8.0 in /home/nirbhay/anaconda3/lib/python3.8/site-packages (from pytorch-lightning==1.1.2) (0.8.3)\n",
      "Requirement already satisfied: torch>=1.3 in /home/nirbhay/anaconda3/lib/python3.8/site-packages (from pytorch-lightning==1.1.2) (1.7.1)\n",
      "Requirement already satisfied: tensorboard>=2.2.0 in /home/nirbhay/anaconda3/lib/python3.8/site-packages (from pytorch-lightning==1.1.2) (2.3.0)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in /home/nirbhay/.local/lib/python3.8/site-packages (from pytorch-lightning==1.1.2) (4.46.1)\n",
      "Requirement already satisfied: typing-extensions in /home/nirbhay/anaconda3/lib/python3.8/site-packages (from torch>=1.3->pytorch-lightning==1.1.2) (3.7.4.3)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /home/nirbhay/.local/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.1.2) (1.18.0)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /home/nirbhay/.local/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.1.2) (3.12.2)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/nirbhay/.local/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.1.2) (0.11.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/nirbhay/anaconda3/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.1.2) (50.3.1.post20201107)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/nirbhay/.local/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.1.2) (1.0.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/nirbhay/.local/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.1.2) (1.7.0)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /home/nirbhay/anaconda3/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.1.2) (0.35.1)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /home/nirbhay/.local/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.1.2) (1.32.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/nirbhay/.local/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.1.2) (2.24.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/nirbhay/anaconda3/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.1.2) (1.15.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/nirbhay/.local/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.1.2) (0.4.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/nirbhay/.local/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.1.2) (3.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/nirbhay/.local/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.1.2) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/nirbhay/.local/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.1.2) (4.1.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /home/nirbhay/.local/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.1.2) (4.6)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/nirbhay/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch-lightning==1.1.2) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/nirbhay/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch-lightning==1.1.2) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/nirbhay/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch-lightning==1.1.2) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/nirbhay/.local/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch-lightning==1.1.2) (1.24.3)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/nirbhay/.local/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.1.2) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/nirbhay/.local/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.1.2) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/nirbhay/anaconda3/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.1.2) (3.1.0)\n",
      "Requirement already satisfied: transformers==4.1.1 in /home/nirbhay/anaconda3/lib/python3.8/site-packages (4.1.1)\n",
      "Requirement already satisfied: numpy in /home/nirbhay/anaconda3/lib/python3.8/site-packages (from transformers==4.1.1) (1.18.5)\n",
      "Requirement already satisfied: tokenizers==0.9.4 in /home/nirbhay/anaconda3/lib/python3.8/site-packages (from transformers==4.1.1) (0.9.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/nirbhay/anaconda3/lib/python3.8/site-packages (from transformers==4.1.1) (2020.10.15)\n",
      "Requirement already satisfied: filelock in /home/nirbhay/anaconda3/lib/python3.8/site-packages (from transformers==4.1.1) (3.0.12)\n",
      "Requirement already satisfied: requests in /home/nirbhay/.local/lib/python3.8/site-packages (from transformers==4.1.1) (2.24.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/nirbhay/.local/lib/python3.8/site-packages (from transformers==4.1.1) (4.46.1)\n",
      "Requirement already satisfied: sacremoses in /home/nirbhay/anaconda3/lib/python3.8/site-packages (from transformers==4.1.1) (0.0.43)\n",
      "Requirement already satisfied: packaging in /home/nirbhay/anaconda3/lib/python3.8/site-packages (from transformers==4.1.1) (20.4)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/nirbhay/anaconda3/lib/python3.8/site-packages (from requests->transformers==4.1.1) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/nirbhay/.local/lib/python3.8/site-packages (from requests->transformers==4.1.1) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/nirbhay/anaconda3/lib/python3.8/site-packages (from requests->transformers==4.1.1) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/nirbhay/anaconda3/lib/python3.8/site-packages (from requests->transformers==4.1.1) (2.10)\n",
      "Requirement already satisfied: six in /home/nirbhay/anaconda3/lib/python3.8/site-packages (from sacremoses->transformers==4.1.1) (1.15.0)\n",
      "Requirement already satisfied: joblib in /home/nirbhay/anaconda3/lib/python3.8/site-packages (from sacremoses->transformers==4.1.1) (0.17.0)\n",
      "Requirement already satisfied: click in /home/nirbhay/anaconda3/lib/python3.8/site-packages (from sacremoses->transformers==4.1.1) (7.1.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/nirbhay/anaconda3/lib/python3.8/site-packages (from packaging->transformers==4.1.1) (2.4.7)\n",
      "Requirement already satisfied: sentencepiece==0.1.94 in /home/nirbhay/anaconda3/lib/python3.8/site-packages (0.1.94)\n",
      "Requirement already satisfied: wandb==0.10.12 in /home/nirbhay/anaconda3/lib/python3.8/site-packages (0.10.12)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /home/nirbhay/anaconda3/lib/python3.8/site-packages (from wandb==0.10.12) (3.1.11)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: watchdog>=0.8.3 in /home/nirbhay/anaconda3/lib/python3.8/site-packages (from wandb==0.10.12) (0.10.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /home/nirbhay/anaconda3/lib/python3.8/site-packages (from wandb==0.10.12) (5.7.2)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /home/nirbhay/.local/lib/python3.8/site-packages (from wandb==0.10.12) (3.12.2)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/nirbhay/anaconda3/lib/python3.8/site-packages (from wandb==0.10.12) (0.4.0)\n",
      "Requirement already satisfied: sentry-sdk>=0.4.0 in /home/nirbhay/anaconda3/lib/python3.8/site-packages (from wandb==0.10.12) (0.19.5)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in /home/nirbhay/anaconda3/lib/python3.8/site-packages (from wandb==0.10.12) (1.0.1)\n",
      "Requirement already satisfied: subprocess32>=3.5.3 in /home/nirbhay/anaconda3/lib/python3.8/site-packages (from wandb==0.10.12) (3.5.4)\n",
      "Requirement already satisfied: Click>=7.0 in /home/nirbhay/anaconda3/lib/python3.8/site-packages (from wandb==0.10.12) (7.1.2)\n",
      "Requirement already satisfied: configparser>=3.8.1 in /home/nirbhay/anaconda3/lib/python3.8/site-packages (from wandb==0.10.12) (5.0.1)\n",
      "Requirement already satisfied: six>=1.13.0 in /home/nirbhay/anaconda3/lib/python3.8/site-packages (from wandb==0.10.12) (1.15.0)\n",
      "Requirement already satisfied: PyYAML in /home/nirbhay/anaconda3/lib/python3.8/site-packages (from wandb==0.10.12) (5.3.1)\n",
      "Requirement already satisfied: promise<3,>=2.0 in /home/nirbhay/anaconda3/lib/python3.8/site-packages (from wandb==0.10.12) (2.3)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /home/nirbhay/.local/lib/python3.8/site-packages (from wandb==0.10.12) (2.24.0)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /home/nirbhay/anaconda3/lib/python3.8/site-packages (from wandb==0.10.12) (2.8.1)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/nirbhay/anaconda3/lib/python3.8/site-packages (from GitPython>=1.0.0->wandb==0.10.12) (4.0.5)\n",
      "Requirement already satisfied: pathtools>=0.1.1 in /home/nirbhay/anaconda3/lib/python3.8/site-packages (from watchdog>=0.8.3->wandb==0.10.12) (0.1.2)\n",
      "Requirement already satisfied: setuptools in /home/nirbhay/anaconda3/lib/python3.8/site-packages (from protobuf>=3.12.0->wandb==0.10.12) (50.3.1.post20201107)\n",
      "Requirement already satisfied: certifi in /home/nirbhay/anaconda3/lib/python3.8/site-packages (from sentry-sdk>=0.4.0->wandb==0.10.12) (2020.6.20)\n",
      "Requirement already satisfied: urllib3>=1.10.0 in /home/nirbhay/.local/lib/python3.8/site-packages (from sentry-sdk>=0.4.0->wandb==0.10.12) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/nirbhay/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb==0.10.12) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/nirbhay/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb==0.10.12) (2.10)\n",
      "Requirement already satisfied: smmap<4,>=3.0.1 in /home/nirbhay/anaconda3/lib/python3.8/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb==0.10.12) (3.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==1.7.1\n",
    "!pip install pytorch-lightning==1.1.2\n",
    "!pip install transformers==4.1.1\n",
    "!pip install sentencepiece==0.1.94\n",
    "!pip install wandb==0.10.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "WB9bvbDfE1od"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers as tfs\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.metrics import Accuracy\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "import json\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from transformers import AdamW, DistilBertForQuestionAnswering, DistilBertTokenizerFast\n",
    "import string, re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Understanding\n",
    "\n",
    "In this section we will import the data & convert it correctly into paralell lists of contexts, questions and answers provided in the SQuAD 2.0 Dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Download SQuAD 2.0 Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : This dataset can be explored in the Hugging Face model hub (SQuAD V2), and can be alternatively downloaded with the 🤗 NLP library with load_dataset(\"squad_v2\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EgIZHZyUDmZc",
    "outputId": "4f60d79c-33fd-4a3c-e3b4-4c0d0ce4971d"
   },
   "outputs": [],
   "source": [
    "## Create a squad directory and download the train and evaluation datasets directly into the library\n",
    "# !mkdir squad\n",
    "# !wget https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json -O squad/train-v2.0.json\n",
    "# !wget https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json -O squad/dev-v2.0.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we will import the data and convert it into parallel lists of contexts, questions, and answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "FQI9qONYDyaV"
   },
   "outputs": [],
   "source": [
    "def read_squad(path):\n",
    "    path = Path(path)\n",
    "    with open(path, 'rb') as f:\n",
    "        squad_dict = json.load(f)\n",
    "\n",
    "    contexts = []\n",
    "    questions = []\n",
    "    answers = []\n",
    "    combined_qac=[] #combined contexts, questions & answers\n",
    "    counter=0\n",
    "    for group in squad_dict['data']:\n",
    "        for passage in group['paragraphs']:\n",
    "            context = passage['context']  \n",
    "            for qa in passage['qas']:\n",
    "                question = qa['question']\n",
    "                q_answers = qa['answers'].copy()\n",
    "                q_answers = list(map(lambda x:x['text'], q_answers))\n",
    "                for answer in qa['answers']:\n",
    "                    contexts.append(context)\n",
    "                    questions.append(question)\n",
    "                    answers.append(answer)\n",
    "                    combined_qac.append({'context':context,'question':question,'answers':q_answers})\n",
    "    return contexts, questions, answers, combined_qac\n",
    "\n",
    "train_contexts, train_questions, train_answers,train_qac = read_squad('squad/train-v2.0.json')\n",
    "val_contexts, val_questions, val_answers, val_qac = read_squad('squad/dev-v2.0.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jkXAOFYYD1El"
   },
   "source": [
    "Now that we have converted the data into parallel lists, let us assess what the dataset holds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86821"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_contexts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'When did Beyonce start becoming popular?'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_questions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'in the late 1990s', 'answer_start': 269}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_answers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86821"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_qac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': 'Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".',\n",
       " 'question': 'When did Beyonce start becoming popular?',\n",
       " 'answers': ['in the late 1990s']}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_qac[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20302"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_contexts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In what country is Normandy located?'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_questions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'France', 'answer_start': 159}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_answers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': 'The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.',\n",
       " 'question': 'In what country is Normandy located?',\n",
       " 'answers': ['France', 'France', 'France', 'France']}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_qac[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations:\n",
    "\n",
    " - We have successfully created 3 subsets of both the training and validation sets\n",
    " - We gathered the following stats:\n",
    "     - **Training Data**\n",
    "         - Length: 86821\n",
    "         - The combined_qac shows the way things will work, i.e.: We submit a context & a question to the model & receive the answer already highlighted\n",
    "         - train_answers shows the answer for a particular question and the start index value\n",
    "     - **Validation Data**\n",
    "         - Length: 20302\n",
    "         - Similar to the train_qac we have created a val_qac to understand the validation dataset better as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will prepare the data appropriately for modelling and training. \n",
    "\n",
    "We will extract token positions where answers begins & ends for train & validation data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The contexts and questions are just strings. The answers are dicts containing the subsequence of the passage with the correct answer as well as an integer indicating the character at which the answer begins. In order to train a model on this data we need (1) the tokenized context/question pairs, and (2) integers indicating at which token positions the answer begins and ends.\n",
    "\n",
    "First, let’s get the character position at which the answer ends in the passage (we are given the starting position). Sometimes SQuAD answers are off by one or two characters, so we will also adjust for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "vB_BJdJ1D8MW"
   },
   "outputs": [],
   "source": [
    "## Index the answers and contexts in the training and validation sets. This will help us generate the tokens \n",
    "## and help get better answers for our questions\n",
    "def add_end_idx(answers, contexts):\n",
    "    for answer, context in zip(answers, contexts):\n",
    "        gold_text = answer['text']\n",
    "        start_idx = answer['answer_start']\n",
    "        end_idx = start_idx + len(gold_text)\n",
    "\n",
    "        # sometimes squad answers are off by a character or two – fix this\n",
    "        if context[start_idx:end_idx] == gold_text:\n",
    "            answer['answer_end'] = end_idx\n",
    "        elif context[start_idx-1:end_idx-1] == gold_text:\n",
    "            answer['answer_start'] = start_idx - 1\n",
    "            answer['answer_end'] = end_idx - 1     # When the gold label is off by one character\n",
    "        elif context[start_idx-2:end_idx-2] == gold_text:\n",
    "            answer['answer_start'] = start_idx - 2\n",
    "            answer['answer_end'] = end_idx - 2     # When the gold label is off by two characters\n",
    "\n",
    "add_end_idx(train_answers, train_contexts)\n",
    "add_end_idx(val_answers, val_contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "AvI7-glYD-EO"
   },
   "outputs": [],
   "source": [
    "## Initialize a tokenizer using DistilBERT which will help us tokenize our training questions and answers\n",
    "\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "## obtain encoded training and validation sets from the tokenizer\n",
    "train_encodings = tokenizer(train_contexts, train_questions, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_contexts, val_questions, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "5smKlK_tEF_m"
   },
   "outputs": [],
   "source": [
    "## Create a function to add token positions\n",
    "def add_token_positions(encodings, answers):\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    for i in range(len(answers)):\n",
    "        start_positions.append(encodings.char_to_token(i, answers[i]['answer_start']))\n",
    "        end_positions.append(encodings.char_to_token(i, answers[i]['answer_end'] - 1))\n",
    "        # if None, the answer passage has been truncated\n",
    "        if start_positions[-1] is None:\n",
    "            start_positions[-1] = tokenizer.model_max_length\n",
    "        if end_positions[-1] is None:\n",
    "            end_positions[-1] = tokenizer.model_max_length\n",
    "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
    "\n",
    "add_token_positions(train_encodings, train_answers)\n",
    "add_token_positions(val_encodings, val_answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "In this section we have successfully taken the split datasets and:\n",
    "\n",
    "    - Added end index postions which helps us identify the correct end values for an answer in a particular\n",
    "      text piece.\n",
    "    - Tokenized our data using DistilBert \n",
    "    - Added token positions to the start and end of the answers based on their encoded positions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is ready. Let’s just put it in a PyTorch dataset so that we can easily use it for training. In PyTorch, we define a custom Dataset class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train & Validation Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "DnnJ_bEmEH6K"
   },
   "outputs": [],
   "source": [
    "## Creating the taining and validation datasets using the encoded training and validation sets we created in\n",
    "## the section above\n",
    "class SquadDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "\n",
    "train_dataset = SquadDataset(train_encodings)\n",
    "val_dataset = SquadDataset(val_encodings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "Our training and validation sets have been successully created. We will now use these to train, validate and score our model below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model Building & Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 171,
     "referenced_widgets": [
      "a315f0960a7648fbafabab34e3e72ac2",
      "0328260a3c9b46e4af190c03f84f4a91",
      "5e9d04345f974608bf56361c233236c6",
      "76faa716ee2b422cbcf482795a7cf33c",
      "6efd58b17b9a498bad39a918041cffa3",
      "34546a54f17f48e09f97bf3c7a1403ef",
      "98d95869e5fc48bba72508eeeb90584a",
      "a0cd27e1521b49ef900d5cae672a352e"
     ]
    },
    "id": "XoCdp27JEMGd",
    "outputId": "88744179-5825-445c-c5a8-7de9ed4e7026"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForQuestionAnswering: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = DistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "We have created a basic model using DistilBert. This model is still not trained. We shall train and validate this model using our available compute unit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "_3vssFehYeh4",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Training the created model using the available cuda gpu or cpu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device) # send the model to the available device for training.\n",
    "model.train()\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=12, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset,batch_size=8,shuffle=False)\n",
    "\n",
    "optim = AdamW(model.parameters(), lr=5e-5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "We have uploaded the base model to our compute device. This helps in faster access to the model, model inputs & outputs while it is being trained. \n",
    "\n",
    "We have also defined our training and validation data loaders with a batch size of 8. A batch size of 8 splits the training into chunks of 8 & uses those chunks for processing together. \n",
    "\n",
    "We chose the AdamW optimizer because it allows us to handle sparse gradient on highly noisy datasets. \n",
    "\n",
    "\n",
    "This was chosen due to compute limitations. A notebook with the batch size of 16 can be found here: https://colab.research.google.com/drive/1dNpiCmNmAKUm8tL3wkW89oV7RxJtwdC1?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "-TwI72oBEJio"
   },
   "outputs": [],
   "source": [
    "# Removing articles and punctuation, and standardizing whitespace are all typical text processing steps\n",
    "\n",
    "\n",
    "def normalize_text(s):\n",
    "\n",
    "    def remove_articles(text):\n",
    "        regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
    "        return re.sub(regex, \" \", text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return \" \".join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return \"\".join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute the exact match for an answer.\n",
    "# This will help us determine how accurately do our answers match with the suggested answers\n",
    "def compute_exact_match(prediction, truth):\n",
    "    return int(normalize_text(prediction) == normalize_text(truth))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute the F1 Statistic \n",
    "\n",
    "def compute_f1(prediction, truth):\n",
    "    pred_tokens = normalize_text(prediction).split()\n",
    "    truth_tokens = normalize_text(truth).split()\n",
    "    \n",
    "    # if either the prediction or the truth is no-answer then f1 = 1 if they agree, 0 otherwise\n",
    "    if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
    "        return int(pred_tokens == truth_tokens)\n",
    "    \n",
    "    common_tokens = set(pred_tokens) & set(truth_tokens)\n",
    "    \n",
    "    # if there are no common tokens then f1 = 0\n",
    "    if len(common_tokens) == 0:\n",
    "        return 0\n",
    "    \n",
    "    prec = len(common_tokens) / len(pred_tokens)\n",
    "    rec = len(common_tokens) / len(truth_tokens)\n",
    "    \n",
    "    return 2 * (prec * rec) / (prec + rec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "O4bMzsBLERpu"
   },
   "outputs": [],
   "source": [
    "# Function to calculate exact match and exact F1 score for a particular training epoch\n",
    "def calculate_stats(input_ids,start,end,idx):\n",
    "    batch_start = 8*idx\n",
    "    batch_end = batch_start+8\n",
    "    data = val_qac[batch_start:batch_end]\n",
    "    em = 0\n",
    "    ef1 = 0\n",
    "    for i,d in enumerate(data):\n",
    "        answer_start = start[i]\n",
    "        answer_end = end[i]\n",
    "        answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[i][answer_start:answer_end]))\n",
    "        gold_ans = d['answers']\n",
    "        if len(gold_ans)==0:\n",
    "            gold_ans.append(\"\")\n",
    "        em_s= max((compute_exact_match(answer, g_answer)) for g_answer in gold_ans)\n",
    "        ef1_s = max((compute_f1(answer, g_answer)) for g_answer in gold_ans)\n",
    "        em+=em_s\n",
    "        ef1+=ef1_s\n",
    "    return em,ef1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a4ZDwxXNEe2T"
   },
   "source": [
    "### Observations\n",
    "\n",
    "Above we have created a few functions that will help us with validation better. \n",
    "\n",
    "The normalize text function allows us to create a uniform text format. It removes punctuations, fixes whitespaces, removes articles and converts everything to lower text. This helps in ensuring that the input and outputs match properly.\n",
    "\n",
    "The calculate stats function returns the exact match and F1 scores for each item we train on. This helps us identiy how well our model is performing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hi8NlLE8EgTL",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/19\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a0f5d9585144c2d95f7cddd2bba350f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7236.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train for the model, perform validation on it per epoch and generate files for a tensorboard\n",
    "num_epochs = 20\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "    print('-' * 10)\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    tk0 = tqdm(train_dataloader, total=int(len(train_dataloader)))    \n",
    "    counter = 0\n",
    "    for idx,batch in enumerate(tk0):\n",
    "        optim.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        start_positions = batch['start_positions'].to(device)\n",
    "        end_positions = batch['end_positions'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        running_loss += loss.item() *  batch['input_ids'].size(0)\n",
    "        counter += 1\n",
    "        tk0.set_postfix(loss=(running_loss / (counter * train_dataloader.batch_size)))\n",
    "    epoch_loss = running_loss / len(train_dataloader)\n",
    "    writer.add_scalar('Train/Loss', epoch_loss,epoch)\n",
    "    print('Training Loss: {:.4f}'.format(epoch_loss))\n",
    "\n",
    "    model.eval()\n",
    "    running_val_loss=0\n",
    "    running_val_em=0\n",
    "    running_val_f1=0\n",
    "    tk1 = tqdm(val_dataloader, total=int(len(val_dataloader)))  \n",
    "    for idx,batch in enumerate(tk1):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        start_positions = batch['start_positions'].to(device)\n",
    "        end_positions = batch['end_positions'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n",
    "        running_val_loss += loss.item() *  batch['input_ids'].size(0)\n",
    "        counter += 1\n",
    "        tk1.set_postfix(loss=(running_loss / (counter * val_dataloader.batch_size)))\n",
    "        answer_start = torch.argmax(outputs['start_logits'], dim=1)  \n",
    "        answer_end = torch.argmax(outputs['end_logits'], dim=1) + 1 \n",
    "        em_score, f1_score = calculate_stats(input_ids,answer_start,answer_end,idx)\n",
    "        running_val_em += em_score\n",
    "        running_val_f1 += f1_score\n",
    "    l = len(val_qac)\n",
    "    epoch_v_loss = running_val_loss /l\n",
    "    epoch_v_em = running_val_em/l\n",
    "    epoch_val_f1 = running_val_f1/l\n",
    "    writer.add_scalar('Val/Loss', epoch_v_loss,epoch)\n",
    "    writer.add_scalar('Val/EM', epoch_v_em,epoch)\n",
    "    writer.add_scalar('Val/F1', epoch_val_f1,epoch)\n",
    "    print('Val Loss: {:.4f}, EM: {:.4f}, F1: {:.4f} '.format(epoch_v_loss,epoch_v_em,epoch_val_f1))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "After running our model training and evaluation for 10 epochs with a batch size of 8 we observe:\n",
    "\n",
    " - The training loss dropped from 12.0271 to 1.8473 in the final epoch. This means that training got better with time.\n",
    " - The validation loss dropped from 2.0035 to 0.0196. This means that our validation also improved with time. \n",
    " - There is a very large difference between the training loss and the validation loss. This highlights that there is underfitting happening. Possible ways to improve this are increase the batch size to 16 and run for a higher number of, epochs to suggest a few. \n",
    " - Our exact match score, EM, suggests that our model is able to get perfect matches, with an F1 score = 1, for 64.21% of our validation set. This is in line with our understanding of the validation dataset that the set has a lot of 'no answer' type questions. \n",
    " - Our best results on the validation set came in the 2nd epoch. The problem with this is that the training loss for that epoch was very high at 7.8%.\n",
    " - We see a consistent decline in the F1 score after the 3rd epoch. The only anamoly is the 8th epoch where the score jumps suddenly. \n",
    " \n",
    " Overall our model has performed well considering the limitations in computing that we have faced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PfNLO7vIGric"
   },
   "outputs": [],
   "source": [
    "# We save our model so that it can be reused later\n",
    "\n",
    "torch.save(model,'./model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kVFpfrIaG0Hc"
   },
   "outputs": [],
   "source": [
    "# Generate a Tensorboard\n",
    "\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "We have created a Tensorboard to map the loss and accuracy across the various epochs that the model has trained at.\n",
    "\n",
    "\n",
    "We will now run some examples to see how our model is performing & is it responding correctly to our questions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Running The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now test the model on some contexts and questions to see if we are getting the correct answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lC-0FgOlHfrI"
   },
   "outputs": [],
   "source": [
    "test_context = \"\"\"The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.\"\"\"\n",
    "\n",
    "\n",
    "test_question = \"\"\"Who was the Norse leader?\"\"\"\n",
    "\n",
    "test_answer =  \"Rollo\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W5nclboJHkP6"
   },
   "outputs": [],
   "source": [
    "def question_answer(question, context, model):\n",
    "    inputs = tokenizer(question,context, return_tensors='pt')\n",
    "\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "    inputs.to(device)\n",
    "    start_scores, end_scores = model(input_ids, attention_mask=attention_mask, output_attentions=False)[:2]\n",
    "\n",
    "    all_tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "    answer = ' '.join(all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores)+1])\n",
    "    answer = tokenizer.convert_tokens_to_ids(answer.split())\n",
    "    answer = tokenizer.decode(answer)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer(test_question, test_context, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking the response is the same as we had in the validation set. \n",
    "question_answer(val_questions[0], val_contexts[0], model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "We can see that our model is performing correctly. As an aditional step below we will load the model again and try to predict the answers for the same questions as we did above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizerFast\n",
    "\n",
    "model_loaded = torch.load('./model.pt')\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lC-0FgOlHfrI"
   },
   "outputs": [],
   "source": [
    "test_context = \"\"\"The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.\"\"\"\n",
    "\n",
    "\n",
    "test_question = \"\"\"Who was the Norse leader?\"\"\"\n",
    "\n",
    "test_answer =  \"Rollo\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W5nclboJHkP6"
   },
   "outputs": [],
   "source": [
    "def question_answer(question, context, model):\n",
    "    inputs = tokenizer(question,context, return_tensors='pt')\n",
    "\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "    inputs.to(device)\n",
    "    start_scores, end_scores = model(input_ids, attention_mask=attention_mask, output_attentions=False)[:2]\n",
    "\n",
    "    all_tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "    answer = ' '.join(all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores)+1])\n",
    "    answer = tokenizer.convert_tokens_to_ids(answer.split())\n",
    "    answer = tokenizer.decode(answer)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer(test_question, test_context, model_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we will now take some text at random from Wikipedia and test our model. This excerpt can be found at:\n",
    "## https://en.wikipedia.org/wiki/Long_short-term_memory under the Idea heading.\n",
    "context = \"\"\"In theory, classic (or \"vanilla\") RNNs can keep track of arbitrary long-term dependencies in the input sequences. The problem with vanilla RNNs is computational (or practical) in nature: when training a vanilla RNN using back-propagation, the gradients which are back-propagated can \"vanish\" (that is, they can tend to zero) or \"explode\" (that is, they can tend to infinity), because of the computations involved in the process, which use finite-precision numbers. RNNs using LSTM units partially solve the vanishing gradient problem, because LSTM units allow gradients to also flow unchanged. However, LSTM networks can still suffer from the exploding gradient problem.\"\"\"\n",
    "question = \"\"\"What problem can LSTM suffer from?\"\"\"\n",
    "answer = \"\"\"exploding gradient problem\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer(question, context, model_loaded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Observations\n",
    "\n",
    "Through this project we have shown the process to process the SQuAD 2.0 dataset and how to use it for building a Question Answering System.\n",
    "\n",
    "We have observed that to be able to successfully create a model that has good accuracy, we need to:\n",
    "- **Prepocess Data** : We need to correctly separate the data into question, answers & contexts so that our model can correctly identify these fields. We need to insert index values to identify the start and end of a context and answer to be able to successfully generate tokens. We then encoded our data and tokenized it, preparing it for modelling. \n",
    "- **Model Creation** : We then built a base model using the pre-trained DistilBert and trained it using PyTorch.\n",
    "- **Training & Validation**: We trained the model and evaluated it using our SQuAD dataset, over 10 epochs with a batch size of 8. Some observations from that are:\n",
    "\n",
    "     - The training loss dropped from 12.0271 to 1.8473 in the final epoch. This means that training got better with time.\n",
    "     - The validation loss dropped from 2.0035 to 0.0196. This means that our validation also improved with time. \n",
    "     - There is a very large difference between the training loss and the validation loss. This highlights that there is underfitting happening. Possible ways to improve this are increase the batch size to 16 and run for a higher number of, epochs to suggest a few. \n",
    "     - Our exact match score, EM, suggests that our model is able to get perfect matches, with an F1 score = 1, for 64.21% of our validation set. This is in line with our understanding of the validation dataset that the set has a lot of 'no answer' type questions. \n",
    "     - Our best results on the validation set came in the 2nd epoch. The problem with this is that the training loss for that epoch was very high at 7.8%.\n",
    "     - We see a consistent decline in the F1 score after the 3rd epoch. The only anamoly is the 8th epoch where the score jumps suddenly. \n",
    "\n",
    "     Overall our model has performed well considering the limitations in computing that we have faced.\n",
    "     \n",
    "     We also ran a separate model in Google Colab with a batch size of 16 for 10 epochs. Link to colab: \n",
    "     https://colab.research.google.com/drive/1dNpiCmNmAKUm8tL3wkW89oV7RxJtwdC1?usp=sharing\n",
    "     \n",
    "- **Testing The Model**: Finally we tested our model using data from the training set as well as a random excerpt from Wiki pedia. In both cases our model is performing well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END OF FILE"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Squad_attempt1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0328260a3c9b46e4af190c03f84f4a91": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "34546a54f17f48e09f97bf3c7a1403ef": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5e9d04345f974608bf56361c233236c6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_34546a54f17f48e09f97bf3c7a1403ef",
      "max": 267967963,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6efd58b17b9a498bad39a918041cffa3",
      "value": 267967963
     }
    },
    "6efd58b17b9a498bad39a918041cffa3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "76faa716ee2b422cbcf482795a7cf33c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a0cd27e1521b49ef900d5cae672a352e",
      "placeholder": "​",
      "style": "IPY_MODEL_98d95869e5fc48bba72508eeeb90584a",
      "value": " 268M/268M [02:26&lt;00:00, 1.83MB/s]"
     }
    },
    "98d95869e5fc48bba72508eeeb90584a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a0cd27e1521b49ef900d5cae672a352e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a315f0960a7648fbafabab34e3e72ac2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5e9d04345f974608bf56361c233236c6",
       "IPY_MODEL_76faa716ee2b422cbcf482795a7cf33c"
      ],
      "layout": "IPY_MODEL_0328260a3c9b46e4af190c03f84f4a91"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
